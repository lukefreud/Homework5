[
  {
    "objectID": "ST558HW5.html",
    "href": "ST558HW5.html",
    "title": "ST 558 Homework 5",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model?\n\n\nThe purpose of using cross-validation for a random forest model is to look into tuning of the folds. You can use it to figure out the best value of the folds to be at which provides the most accurate model.\n\n\nDescribe the bagged tree algorithm.\n\n\nA bagged tree algorithm takes multiple random samples (or bootstraps) of the original data set. It would then train a model on that sample from the original data set. The final prediction values are the average of all of these predictions from the bootstraps.\n\n\nWhat is meant by a general linear model?\n\n\nA general linear model is a broader type of linear model that has a continuous response variable and allows for both continuous and categorical predictors.\n\n\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\n\n\nAdding an interaction term into a model allows you to examine how a certain predictor behaves under different values of another variable.\n\n\nWhy do we split our data into a training and test set?\n\n\nWe split the data into a training and test set so that we can test our models in terms of accuracy. We are able to train our models using the training set and then test their predictions they make on the test set."
  },
  {
    "objectID": "ST558HW5.html#reading-in-the-data",
    "href": "ST558HW5.html#reading-in-the-data",
    "title": "ST 558 Homework 5",
    "section": "Reading in the Data",
    "text": "Reading in the Data\n\nheart_data &lt;- read_csv(\"heart.csv\")\n\nRows: 918 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope\ndbl (7): Age, RestingBP, Cholesterol, FastingBS, MaxHR, Oldpeak, HeartDisease\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(heart_data)\n\n# A tibble: 6 × 12\n    Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1    40 M     ATA                 140         289         0 Normal       172\n2    49 F     NAP                 160         180         0 Normal       156\n3    37 M     ATA                 130         283         0 ST            98\n4    48 F     ASY                 138         214         0 Normal       108\n5    54 M     NAP                 150         195         0 Normal       122\n6    39 M     NAP                 120         339         0 Normal       170\n# ℹ 4 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, ST_Slope &lt;chr&gt;,\n#   HeartDisease &lt;dbl&gt;"
  },
  {
    "objectID": "ST558HW5.html#exploring-the-data-set",
    "href": "ST558HW5.html#exploring-the-data-set",
    "title": "ST 558 Homework 5",
    "section": "Exploring the Data Set",
    "text": "Exploring the Data Set\nWe will now explore some summaries of our data set. We will look at missingness in values along with relationships of variables to HeartDisease (our response).\n\n# Checking on Missingness in Data Set\nsum(is.na(heart_data))\n\n[1] 0\n\n# Checking out summary of data and relationships\nsummary(heart_data)\n\n      Age            Sex            ChestPainType        RestingBP    \n Min.   :28.00   Length:918         Length:918         Min.   :  0.0  \n 1st Qu.:47.00   Class :character   Class :character   1st Qu.:120.0  \n Median :54.00   Mode  :character   Mode  :character   Median :130.0  \n Mean   :53.51                                         Mean   :132.4  \n 3rd Qu.:60.00                                         3rd Qu.:140.0  \n Max.   :77.00                                         Max.   :200.0  \n  Cholesterol      FastingBS       RestingECG            MaxHR      \n Min.   :  0.0   Min.   :0.0000   Length:918         Min.   : 60.0  \n 1st Qu.:173.2   1st Qu.:0.0000   Class :character   1st Qu.:120.0  \n Median :223.0   Median :0.0000   Mode  :character   Median :138.0  \n Mean   :198.8   Mean   :0.2331                      Mean   :136.8  \n 3rd Qu.:267.0   3rd Qu.:0.0000                      3rd Qu.:156.0  \n Max.   :603.0   Max.   :1.0000                      Max.   :202.0  \n ExerciseAngina        Oldpeak          ST_Slope          HeartDisease   \n Length:918         Min.   :-2.6000   Length:918         Min.   :0.0000  \n Class :character   1st Qu.: 0.0000   Class :character   1st Qu.:0.0000  \n Mode  :character   Median : 0.6000   Mode  :character   Median :1.0000  \n                    Mean   : 0.8874                      Mean   :0.5534  \n                    3rd Qu.: 1.5000                      3rd Qu.:1.0000  \n                    Max.   : 6.2000                      Max.   :1.0000  \n\n\nWe will now look at some relationships between variables to our response variable. First, we will look at a contingency table of sex vs. heart disease.\n\ntable(heart_data$Sex, heart_data$HeartDisease)\n\n   \n      0   1\n  F 143  50\n  M 267 458\n\n\nWe will now look at a couple of correlations. We will look at the correlation between resting BP and heart disease, and cholesterol level and heart disease.\n\n# Correlation between resting BP and Heart Disease\ncor(heart_data$RestingBP, heart_data$HeartDisease)\n\n[1] 0.107589\n\n# Correlation between cholesterol and Heart Disease\ncor(heart_data$Cholesterol, heart_data$HeartDisease)\n\n[1] -0.2327406\n\n\nNext, we will create a factor version of the HeartDisease variable and remove unnecessary variables in our data set.\n\nheart_data &lt;- heart_data |&gt;\n  mutate(HeartDiseaseFactor = as.factor(HeartDisease)) |&gt;\n  select(-c(HeartDisease, ST_Slope))\nheart_data\n\n# A tibble: 918 × 11\n     Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1    40 M     ATA                 140         289         0 Normal       172\n 2    49 F     NAP                 160         180         0 Normal       156\n 3    37 M     ATA                 130         283         0 ST            98\n 4    48 F     ASY                 138         214         0 Normal       108\n 5    54 M     NAP                 150         195         0 Normal       122\n 6    39 M     NAP                 120         339         0 Normal       170\n 7    45 F     ATA                 130         237         0 Normal       170\n 8    54 M     ATA                 110         208         0 Normal       142\n 9    37 M     ASY                 140         207         0 Normal       130\n10    48 F     ATA                 120         284         0 Normal       120\n# ℹ 908 more rows\n# ℹ 3 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;,\n#   HeartDiseaseFactor &lt;fct&gt;\n\n\nWe will now create dummy variables from the categorical predictors.\n\n# Creating a set of dummy variables\ndummies &lt;- dummyVars( ~ Sex + ExerciseAngina + ChestPainType + RestingECG, data = heart_data)\n# Using the syntax given in the caret file to show dummy variables\ndf &lt;- as.data.frame(predict(dummies, newdata = heart_data))\n# Binding the data frames into the new data\nheart_data_new &lt;- cbind(heart_data, df)\nhead(heart_data_new)\n\n  Age Sex ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n1  40   M           ATA       140         289         0     Normal   172\n2  49   F           NAP       160         180         0     Normal   156\n3  37   M           ATA       130         283         0         ST    98\n4  48   F           ASY       138         214         0     Normal   108\n5  54   M           NAP       150         195         0     Normal   122\n6  39   M           NAP       120         339         0     Normal   170\n  ExerciseAngina Oldpeak HeartDiseaseFactor SexF SexM ExerciseAnginaN\n1              N     0.0                  0    0    1               1\n2              N     1.0                  1    1    0               1\n3              N     0.0                  0    0    1               1\n4              Y     1.5                  1    1    0               0\n5              N     0.0                  0    0    1               1\n6              N     0.0                  0    0    1               1\n  ExerciseAnginaY ChestPainTypeASY ChestPainTypeATA ChestPainTypeNAP\n1               0                0                1                0\n2               0                0                0                1\n3               0                0                1                0\n4               1                1                0                0\n5               0                0                0                1\n6               0                0                0                1\n  ChestPainTypeTA RestingECGLVH RestingECGNormal RestingECGST\n1               0             0                1            0\n2               0             0                1            0\n3               0             0                0            1\n4               0             0                1            0\n5               0             0                1            0\n6               0             0                1            0"
  },
  {
    "objectID": "ST558HW5.html#classification-tree-model",
    "href": "ST558HW5.html#classification-tree-model",
    "title": "ST 558 Homework 5",
    "section": "Classification Tree Model",
    "text": "Classification Tree Model\nFirst we will fit a classification tree model on our data set.\n\nclassification_Tree &lt;- train(HeartDiseaseFactor ~ Age + Sex + MaxHR + Cholesterol + ExerciseAngina, data = HeartTrain, \n                 method = \"rpart\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = expand.grid(cp = seq(0,0.1, by=0.001)))\nclassification_Tree\n\nCART \n\n734 samples\n  5 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 661, 660, 661, 661, 661, 661, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.7575680  0.5028460\n  0.001  0.7616652  0.5111529\n  0.002  0.7675644  0.5243938\n  0.003  0.7711804  0.5327793\n  0.004  0.7753023  0.5429575\n  0.005  0.7766783  0.5460549\n  0.006  0.7816642  0.5571188\n  0.007  0.7839226  0.5622383\n  0.008  0.7839226  0.5622383\n  0.009  0.7793749  0.5529667\n  0.010  0.7789121  0.5527117\n  0.011  0.7679899  0.5303633\n  0.012  0.7679961  0.5319383\n  0.013  0.7702545  0.5399204\n  0.014  0.7775049  0.5547890\n  0.015  0.7775049  0.5548868\n  0.016  0.7775049  0.5548868\n  0.017  0.7788747  0.5578573\n  0.018  0.7802261  0.5602219\n  0.019  0.7802261  0.5602219\n  0.020  0.7802261  0.5602219\n  0.021  0.7802261  0.5602219\n  0.022  0.7802261  0.5602219\n  0.023  0.7802261  0.5602219\n  0.024  0.7802261  0.5602219\n  0.025  0.7802261  0.5602219\n  0.026  0.7802261  0.5602219\n  0.027  0.7802261  0.5602219\n  0.028  0.7802261  0.5602219\n  0.029  0.7802261  0.5602219\n  0.030  0.7802261  0.5602219\n  0.031  0.7802261  0.5602219\n  0.032  0.7802261  0.5602219\n  0.033  0.7802261  0.5602219\n  0.034  0.7802261  0.5602219\n  0.035  0.7802261  0.5602219\n  0.036  0.7802261  0.5602219\n  0.037  0.7802261  0.5602219\n  0.038  0.7802261  0.5602219\n  0.039  0.7802261  0.5602219\n  0.040  0.7802261  0.5602219\n  0.041  0.7802261  0.5602219\n  0.042  0.7802261  0.5602219\n  0.043  0.7802261  0.5602219\n  0.044  0.7802261  0.5602219\n  0.045  0.7802261  0.5602219\n  0.046  0.7802261  0.5602219\n  0.047  0.7802261  0.5602219\n  0.048  0.7802261  0.5602219\n  0.049  0.7802261  0.5602219\n  0.050  0.7802261  0.5602219\n  0.051  0.7802261  0.5602219\n  0.052  0.7802261  0.5602219\n  0.053  0.7802261  0.5602219\n  0.054  0.7802261  0.5602219\n  0.055  0.7802261  0.5602219\n  0.056  0.7802261  0.5602219\n  0.057  0.7802261  0.5602219\n  0.058  0.7802261  0.5602219\n  0.059  0.7802261  0.5602219\n  0.060  0.7802261  0.5602219\n  0.061  0.7802261  0.5602219\n  0.062  0.7802261  0.5602219\n  0.063  0.7802261  0.5602219\n  0.064  0.7802261  0.5602219\n  0.065  0.7802261  0.5602219\n  0.066  0.7802261  0.5602219\n  0.067  0.7802261  0.5602219\n  0.068  0.7802261  0.5602219\n  0.069  0.7802261  0.5602219\n  0.070  0.7802261  0.5602219\n  0.071  0.7802261  0.5602219\n  0.072  0.7802261  0.5602219\n  0.073  0.7802261  0.5602219\n  0.074  0.7802261  0.5602219\n  0.075  0.7802261  0.5602219\n  0.076  0.7802261  0.5602219\n  0.077  0.7802261  0.5602219\n  0.078  0.7802261  0.5602219\n  0.079  0.7802261  0.5602219\n  0.080  0.7802261  0.5602219\n  0.081  0.7802261  0.5602219\n  0.082  0.7802261  0.5602219\n  0.083  0.7802261  0.5602219\n  0.084  0.7802261  0.5602219\n  0.085  0.7802261  0.5602219\n  0.086  0.7802261  0.5602219\n  0.087  0.7802261  0.5602219\n  0.088  0.7802261  0.5602219\n  0.089  0.7802261  0.5602219\n  0.090  0.7802261  0.5602219\n  0.091  0.7802261  0.5602219\n  0.092  0.7802261  0.5602219\n  0.093  0.7802261  0.5602219\n  0.094  0.7802261  0.5602219\n  0.095  0.7802261  0.5602219\n  0.096  0.7802261  0.5602219\n  0.097  0.7802261  0.5602219\n  0.098  0.7802261  0.5602219\n  0.099  0.7802261  0.5602219\n  0.100  0.7802261  0.5602219\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.008.\n\n\nThe turning parameter with the highest accuracy was cp = .006. We will use this to make predictions and a confusion matrix on our test set.\n\nCT_predictions &lt;- predict(classification_Tree, newdata = HeartTest)\nconfusionMatrix(data = HeartTest$HeartDiseaseFactor, reference = LR_predictions)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 65 24\n         1 21 74\n                                          \n               Accuracy : 0.7554          \n                 95% CI : (0.6868, 0.8157)\n    No Information Rate : 0.5326          \n    P-Value [Acc &gt; NIR] : 3.87e-10        \n                                          \n                  Kappa : 0.5098          \n                                          \n Mcnemar's Test P-Value : 0.7656          \n                                          \n            Sensitivity : 0.7558          \n            Specificity : 0.7551          \n         Pos Pred Value : 0.7303          \n         Neg Pred Value : 0.7789          \n             Prevalence : 0.4674          \n         Detection Rate : 0.3533          \n   Detection Prevalence : 0.4837          \n      Balanced Accuracy : 0.7555          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nThe classification tree model had 75.54% accuracy in predicting the test set."
  },
  {
    "objectID": "ST558HW5.html#random-forest-model",
    "href": "ST558HW5.html#random-forest-model",
    "title": "ST 558 Homework 5",
    "section": "Random Forest Model",
    "text": "Random Forest Model\nNext, we will fit a random forest model on our training set.\n\nrandomForestModel &lt;- train(HeartDiseaseFactor ~ Age + Sex + MaxHR + Cholesterol + ExerciseAngina, data = HeartTrain, \n                 method = \"rf\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = data.frame(mtry = 1:5))\nrandomForestModel\n\nRandom Forest \n\n734 samples\n  5 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 661, 661, 660, 661, 660, 660, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n  1     0.7961496  0.5861832\n  2     0.7865605  0.5661416\n  3     0.7665926  0.5246193\n  4     0.7597865  0.5119450\n  5     0.7593114  0.5112639\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 1.\n\n\nThe turning parameter with the highest accuracy was mtry = 1. We will use this to make predictions and a confusion matrix on our test set.\n\nRF_predictions &lt;- predict(randomForestModel, newdata = HeartTest)\nconfusionMatrix(data = HeartTest$HeartDiseaseFactor, reference = RF_predictions)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 75 14\n         1 17 78\n                                          \n               Accuracy : 0.8315          \n                 95% CI : (0.7695, 0.8826)\n    No Information Rate : 0.5             \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.663           \n                                          \n Mcnemar's Test P-Value : 0.7194          \n                                          \n            Sensitivity : 0.8152          \n            Specificity : 0.8478          \n         Pos Pred Value : 0.8427          \n         Neg Pred Value : 0.8211          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4076          \n   Detection Prevalence : 0.4837          \n      Balanced Accuracy : 0.8315          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nThis random forest model had 83.15% accuracy on predicting the test set."
  },
  {
    "objectID": "ST558HW5.html#boosted-tree-model",
    "href": "ST558HW5.html#boosted-tree-model",
    "title": "ST 558 Homework 5",
    "section": "Boosted Tree Model",
    "text": "Boosted Tree Model\nNext, we will fit a boosted tree model on our training set.\n\nBoostedTreeModel &lt;- train(HeartDiseaseFactor ~ Age + Sex + MaxHR + Cholesterol + ExerciseAngina, data = HeartTrain, \n                 method = \"gbm\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = expand.grid(n.trees = c(25,50,100,200),\n                                        interaction.depth = c(1,2,3),\n                                        shrinkage = 0.1,\n                                        n.minobsinnode = 10),\n                 verbose = FALSE)\nBoostedTreeModel\n\nStochastic Gradient Boosting \n\n734 samples\n  5 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 660, 661, 661, 661, 661, 661, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   25      0.7864899  0.5680560\n  1                   50      0.7824111  0.5591472\n  1                  100      0.7805851  0.5551270\n  1                  200      0.7774440  0.5486440\n  2                   25      0.7801160  0.5544814\n  2                   50      0.7792151  0.5516571\n  2                  100      0.7787648  0.5496417\n  2                  200      0.7746986  0.5408297\n  3                   25      0.7837565  0.5600954\n  3                   50      0.7828681  0.5576231\n  3                  100      0.7764876  0.5446340\n  3                  200      0.7674227  0.5261068\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 25, interaction.depth =\n 1, shrinkage = 0.1 and n.minobsinnode = 10.\n\n\nThe turning parameters with the highest accuracy were n.trees = 25 and interaction.depth = 1. We will use this to make predictions and a confusion matrix on our test set.\n\nBoosted_predictions &lt;- predict(BoostedTreeModel, newdata = HeartTest)\nconfusionMatrix(data = HeartTest$HeartDiseaseFactor, reference = Boosted_predictions)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 75 14\n         1 15 80\n                                          \n               Accuracy : 0.8424          \n                 95% CI : (0.7816, 0.8918)\n    No Information Rate : 0.5109          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6846          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.8333          \n            Specificity : 0.8511          \n         Pos Pred Value : 0.8427          \n         Neg Pred Value : 0.8421          \n             Prevalence : 0.4891          \n         Detection Rate : 0.4076          \n   Detection Prevalence : 0.4837          \n      Balanced Accuracy : 0.8422          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nThis boosted model had 84.24% accuracy on predicting the test set."
  }
]